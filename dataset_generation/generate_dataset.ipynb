{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.all import *\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target number of graph in the dataset (~300k)\n",
    "target = 300000\n",
    "# bucket size for decimal invariants\n",
    "float_value_range = 0.5\n",
    "# every graph having score < threshold is discarded\n",
    "threshold = 25.5\n",
    "\n",
    "test_limit = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diameter(G):\n",
    "    d = Graph(G).diameter()\n",
    "    return d if not math.isinf(d) else -1\n",
    "\n",
    "def spectral_radius_lm(G):\n",
    "    real_value = max(abs(np.linalg.eigvals(nx.laplacian_matrix(G).toarray())))\n",
    "    return round(real_value / float_value_range) * float_value_range\n",
    "\n",
    "def spectral_radius_am(G):\n",
    "    real_value = max(abs(np.linalg.eigvals(nx.adjacency_matrix(G).toarray())))\n",
    "    return round(real_value / float_value_range) * float_value_range\n",
    "\n",
    "funcs = {\n",
    "    'nodes_count': lambda G : len(Graph(G)),\n",
    "    'diameter': diameter,\n",
    "    'matching_number': lambda G : len(Graph(G).matching()),\n",
    "    'clique_number': lambda G : Graph(G).clique_number(),\n",
    "    'independence_number': lambda G : len(Graph(G).independent_set()),\n",
    "    'spectral_radius_laplacian': spectral_radius_lm,\n",
    "    'spectral_radius_adjacency': spectral_radius_am\n",
    "}\n",
    "\n",
    "invariant_occurrences = { key: {} for key in funcs.keys() }\n",
    "\n",
    "graph_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Valid graph ratio at iteration 200.000: 10%  \n",
    "def Gnp():\n",
    "    p = scipy.stats.beta.rvs(0.4, 0.4)  # Beta distribution with higher probability at the extremes\n",
    "    return nx.fast_gnp_random_graph(n=random.randint(9, 50), p=p)\n",
    "\n",
    "#  Valid graph ratio at iteration 200.000: 40%  \n",
    "def GnpLow():\n",
    "    p = scipy.stats.beta.rvs(2, 5)\n",
    "    return nx.fast_gnp_random_graph(n=random.randint(9, 50), p=p)\n",
    "\n",
    "#  Valid graph ratio at iteration 200.000: 40%  \n",
    "def GnpHigh():\n",
    "    p = scipy.stats.beta.rvs(5, 2)\n",
    "    return nx.fast_gnp_random_graph(n=random.randint(9, 50), p=p)\n",
    "\n",
    "#  Valid graph ratio at iteration 200.000: 30%  \n",
    "def BarabasiAlbert():\n",
    "    nodes = random.randint(9, 50)\n",
    "    return nx.barabasi_albert_graph(n=nodes, m=random.randint(1, nodes - 1))\n",
    "\n",
    "#  Valid graph ratio at iteration 200.000: 17%  \n",
    "def RandomTree():\n",
    "    return nx.random_tree(n=random.randint(9, 50))\n",
    "\n",
    "#  Valid graph ratio at iteration 200.000: 35%  \n",
    "def Watts():\n",
    "    n = random.randint(9, 50)  \n",
    "    k = random.randint(2, min(n-1, 10))\n",
    "    p = random.uniform(0.05, 1)\n",
    "    return nx.watts_strogatz_graph(n, k, p)\n",
    "\n",
    "def Caveman():\n",
    "    return  nx.caveman_graph(l=random.randint(2, 10), k=random.randint(2, 10))\n",
    "\n",
    "def Cycle():\n",
    "    return nx.cycle_graph(n=random.randint(9, 50))\n",
    "\n",
    "def Grid():\n",
    "    m = random.randint(2, 10)\n",
    "    n = random.randint(2, int(50/m))\n",
    "    return nx.grid_2d_graph(m, n)\n",
    "\n",
    "#  Valid graph ratio at iteration 200.000: 45%  \n",
    "def StochasticBlock():\n",
    "    total_nodes = random.randint(9, 50)\n",
    "    num_groups = random.randint(2, 9)\n",
    "    \n",
    "    # Random division of nodes in groups\n",
    "    alpha = np.ones(num_groups)     # vector of ones, with length equal to num_groups\n",
    "    p = np.random.dirichlet(alpha)  # probability vector from the Dirichlet distribution\n",
    "    sizes = np.random.multinomial(total_nodes, p)\n",
    "\n",
    "    # Probs matrix, low dentro groups, high tra groups\n",
    "    probs = np.full((num_groups, num_groups), random.uniform(0.05, 0.3))  \n",
    "    np.fill_diagonal(probs, [random.uniform(0.5, 0.9) for _ in range(num_groups)])  \n",
    "\n",
    "    return nx.stochastic_block_model(sizes, probs)\n",
    "\n",
    "#  Valid graph ratio at iteration 200.000: 6%  \n",
    "def Waxman():\n",
    "    p_beta = scipy.stats.beta.rvs(2, 5)  # higher probability 0-0.3\n",
    "    p_alpha = scipy.stats.beta.rvs(4, 8) #higher probability 0.2-0.4 \n",
    "    return nx.waxman_graph(n=random.randint(9, 50), beta=p_beta, alpha=p_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hash_function(self):\n",
    "    hash_str = nx.weisfeiler_lehman_graph_hash(G=self)\n",
    "    return hash(hash_str)\n",
    "\n",
    "nx.Graph.__hash__ = graph_hash_function\n",
    "\n",
    "def save_checkpoint(graph_list, invariant_occurrences, global_scores, accepted_mask, checkpoint_dir, index, hash_set = None):\n",
    "    with open(os.path.join(checkpoint_dir, 'graph-set', f'graph_set_{index}.pkl'), 'wb') as f:\n",
    "        pickle.dump(graph_list, file=f)\n",
    "    with open(os.path.join(checkpoint_dir, 'invariant_occurrences.pkl'), 'wb') as f:\n",
    "        pickle.dump(invariant_occurrences, file=f)\n",
    "    with open(os.path.join(checkpoint_dir, 'global_scores.pkl'), 'wb') as f:\n",
    "        pickle.dump(global_scores, file=f)\n",
    "    with open(os.path.join(checkpoint_dir, 'accepted_mask.pkl'), 'wb') as f:\n",
    "        pickle.dump(accepted_mask, file=f)\n",
    "    if hash_set is not None:\n",
    "        with open(os.path.join(checkpoint_dir, 'hash_set.pkl'), 'wb') as f:\n",
    "            pickle.dump(hash_set, file=f)\n",
    "\n",
    "\n",
    "def restore_checkpoint(checkpoint_dir):\n",
    "    restored_list = []\n",
    "    invaraint_occurrences = {}\n",
    "    global_scores = []\n",
    "    accepted_mask = []\n",
    "    hash_set = set()\n",
    "\n",
    "    with open(os.path.join(checkpoint_dir, 'invariant_occurrences.pkl'), 'rb') as f:\n",
    "        invaraint_occurrences = pickle.load(file=f)\n",
    "\n",
    "    with open(os.path.join(checkpoint_dir, 'global_scores.pkl'), 'rb') as f:\n",
    "        global_scores = pickle.load(file=f)\n",
    "\n",
    "    with open(os.path.join(checkpoint_dir, 'accepted_mask.pkl'), 'rb') as f:\n",
    "        accepted_mask = pickle.load(file=f)\n",
    "    \n",
    "    with open(os.path.join(checkpoint_dir, 'hash_set.pkl'), 'rb') as f:\n",
    "        hash_set = pickle.load(file=f)\n",
    "    \n",
    "    graph_set_dir_path = os.path.join(checkpoint_dir, 'graph-set')\n",
    "    for file_name in os.listdir(graph_set_dir_path):\n",
    "        with open(os.path.join(graph_set_dir_path, file_name), 'rb') as f:\n",
    "            partial_list = pickle.load(file=f)\n",
    "            restored_list.extend(partial_list)\n",
    "    \n",
    "    return restored_list, invaraint_occurrences, global_scores, accepted_mask, hash_set\n",
    "\n",
    "\n",
    "def calc_invariants(graph):\n",
    "    return { key: funcs[key](graph) for key in funcs.keys() }\n",
    "\n",
    "\n",
    "def calc_invariant_score(invariant, value, total_count, invaraint_occ):\n",
    "    occurrences = invaraint_occ[invariant].get(value, sys.float_info.epsilon)\n",
    "    invariant_freq =  occurrences / (total_count or 1)\n",
    "    invariant_score = -math.log(invariant_freq)\n",
    "    return invariant_score\n",
    "\n",
    "\n",
    "def calc_score(graph, total_cont, invariant_occ):\n",
    "    values = calc_invariants(graph)\n",
    "    score = 0\n",
    "    partial_scores = {}\n",
    "    for invariant in funcs.keys():\n",
    "        invariant_value = values[invariant]\n",
    "        partial_scores[invariant] = calc_invariant_score(invariant=invariant, value=invariant_value, total_count=total_cont, invaraint_occ=invariant_occ)\n",
    "        score += partial_scores[invariant]\n",
    "    return score, partial_scores, values\n",
    "\n",
    "\n",
    "def add_graph(graph, invariant_values):\n",
    "    if graph in graph_set:\n",
    "        return\n",
    "    graph_set.add(graph)\n",
    "    for inv in funcs.keys():\n",
    "        value = invariant_values[inv]\n",
    "        if value in invariant_occurrences[inv]:\n",
    "            invariant_occurrences[inv][value] += 1\n",
    "        else:\n",
    "            invariant_occurrences[inv][value] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../plots\"\n",
    "\n",
    "def movingaverage(values, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    avg = np.convolve(values, window, 'same')\n",
    "    return avg\n",
    "\n",
    "\n",
    "def plot_partial_scores(scorelist, filename, title, sections=None):\n",
    "    fig, axs = plt.subplots(len(list(scorelist.keys())), 1, figsize=(9, 14), layout=\"constrained\", sharex=True)\n",
    "    plt.xlabel(f\"Iteration\")\n",
    "    plt.ylabel(\"Score value\")\n",
    "    plt.suptitle(title)\n",
    "    for i, key in enumerate(list(scorelist.keys())):\n",
    "        values = np.array(scorelist[key])\n",
    "        x = np.arange(len(scorelist[key]))\n",
    "        y = values\n",
    "        \n",
    "        if sections is not None:\n",
    "            x_sections = np.split(x, sections)\n",
    "            y_sections = np.split(y, sections)\n",
    "            x_y_sections = zip(x_sections, y_sections)\n",
    "            for x_y_section in x_y_sections:\n",
    "                axs[i].plot(x_y_section[0], x_y_section[1], label=key, linewidth=0.6)\n",
    "                axs[i].set_ylabel(f\"{key} score\", fontsize=6)\n",
    "        else:\n",
    "            axs[i].plot(x, y, label=key, linewidth=0.6)\n",
    "            axs[i].set_ylabel(f\"{key} score\", fontsize=6)\n",
    "        \n",
    "    plt.savefig(os.path.join(dir_path, filename))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_general_score(scorelist, filename, validcount, avgwindow, title, sections=None, sectionnames=None):\n",
    "    values = np.array(scorelist['general'])\n",
    "    x = np.arange(len(values))\n",
    "    y = values\n",
    "    y_avg = movingaverage(y, avgwindow)\n",
    "    avg_padding = math.ceil(avgwindow/2)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.xlabel(f\"Iteration\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    if sections is not None:\n",
    "        x_sections = np.split(x, sections)\n",
    "        y_sections = np.split(y, sections)\n",
    "        x_y_sections = zip(x_sections, y_sections)\n",
    "        for j, x_y_section in enumerate(x_y_sections):\n",
    "            plt.plot(x_y_section[0], x_y_section[1], label=sectionnames[j] or j, linewidth=0.6)\n",
    "    else:\n",
    "        plt.plot(x, y, label='score', linewidth=0.6)\n",
    "\n",
    "    # plt.hlines(y=threshold, xmin=0, xmax=test_limit, linewidth=1, color='r', linestyle=':')\n",
    "    plt.scatter(x[y > y_avg], y[y > y_avg] + 4, color='red', s=10, marker='v', label=f'valid graphs: {validcount}')\n",
    "    plt.plot(x[avg_padding:-avg_padding], y_avg[avg_padding:-avg_padding], label='moving average', linewidth=1.2)\n",
    "    # plt.ylim((0, 30)) \n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(label=title)\n",
    "    plt.savefig(os.path.join(dir_path, filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = [\n",
    "    #Gnp,\n",
    "    GnpLow,     \n",
    "    GnpHigh,\n",
    "    BarabasiAlbert,\n",
    "    RandomTree,\n",
    "    Watts,\n",
    "    #Caveman,   #  Valid graph ratio at iteration 200.000: 0%  \n",
    "    #Cycle,     #  Valid graph ratio at iteration 200.000: 0%  \n",
    "    #Grid,      #  Valid graph ratio at iteration 200.000: 0%  \n",
    "    StochasticBlock,\n",
    "    Waxman\n",
    "]\n",
    "\n",
    "window_size = 100\n",
    "\n",
    "def min_score(values, w_size):\n",
    "    padded_values = np.pad(values, (w_size, 0), 'constant', constant_values=(0, 0))\n",
    "    window = padded_values[-w_size:]\n",
    "    avg = np.mean(window)\n",
    "    return min(avg, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for generator in generators:\n",
    "    graph_set.clear()\n",
    "    invariant_occurrences = { key: {} for key in funcs.keys() }\n",
    "    score_list = { key: [] for key in (list(funcs.keys()) + ['general']) }\n",
    "    i = 0\n",
    "\n",
    "    while (len(graph_set) < target) and (i < test_limit):\n",
    "        g = generator()\n",
    "        score, partial_scores, values = calc_score(g, total_cont=len(graph_set), invariant_occ=invariant_occurrences)\n",
    "\n",
    "        if score >= min_score(score_list['general'], w_size=window_size):\n",
    "            add_graph(g, invariant_values=values)\n",
    "            \n",
    "        for inv in funcs.keys():\n",
    "            score_list[inv].append(partial_scores[inv])\n",
    "        score_list['general'].append(score)\n",
    "        i += 1\n",
    "\n",
    "    plot_partial_scores(scorelist=score_list, filename=f\"partial_score_{generator.__name__}.pdf\", title=generator.__name__)\n",
    "    plot_general_score(scorelist=score_list, filename=f\"general_score_{generator.__name__}.pdf\", validcount=len(graph_set), avgwindow=window_size, title=generator.__name__)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_set.clear()\n",
    "invariant_occurrences = { key: {} for key in funcs.keys() }\n",
    "score_list = { key: [] for key in (list(funcs.keys()) + ['general']) }\n",
    "i = 0\n",
    "\n",
    "section_limit = math.floor(test_limit/len(generators))\n",
    "\n",
    "for generator in generators:\n",
    "    i = 0\n",
    "    while (len(graph_set) < target) and (i < section_limit):\n",
    "        g = generator()\n",
    "        score, partial_scores, values = calc_score(g, total_cont=len(graph_set), invariant_occ=invariant_occurrences)\n",
    "\n",
    "        if score >= min_score(score_list['general'], w_size=window_size):\n",
    "            add_graph(g, invariant_values=values)\n",
    "        \n",
    "        for inv in funcs.keys():\n",
    "            score_list[inv].append(partial_scores[inv])\n",
    "        score_list['general'].append(score)\n",
    "        i += 1\n",
    "\n",
    "plot_partial_scores(scorelist=score_list, filename=f\"partial_score_generators.pdf\", title=\"Generators combined\", sections=[section_limit * i for i in range(1, len(generators))])\n",
    "plot_general_score(scorelist=score_list, filename=f\"general_score_generators.pdf\", validcount=len(graph_set), avgwindow=window_size, title=\"Generators combined\", sections=[section_limit * i for i in range(1, len(generators))], sectionnames=[gen.__name__ for gen in generators])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = []\n",
    "hash_set = set()\n",
    "invariant_occurrences = {key: {} for key in funcs.keys()}\n",
    "score_list = {key: [] for key in (list(funcs.keys()) + ['general'])}\n",
    "\n",
    "global_scores = []\n",
    "accepted_mask = []\n",
    "already_existing_count = 0\n",
    "\n",
    "i = 0\n",
    "checkpoint_index = 0\n",
    "generator_index = 0\n",
    "generator = generators[0]\n",
    "\n",
    "def add_graph_list(graph, invariant_values) -> bool:\n",
    "    hash_str = nx.weisfeiler_lehman_graph_hash(G=graph)\n",
    "    if hash_str in hash_set:\n",
    "        return false\n",
    "    graph_list.append(graph)\n",
    "    hash_set.add(hash_str)\n",
    "    for inv in funcs.keys():\n",
    "        value = invariant_values[inv]\n",
    "        if value in invariant_occurrences[inv]:\n",
    "            invariant_occurrences[inv][value] += 1\n",
    "        else:\n",
    "            invariant_occurrences[inv][value] = 1\n",
    "    return true\n",
    "\n",
    "start = time.time()\n",
    "old_valid_count = 0\n",
    "\n",
    "with open('out.txt', 'w+') as out_file:\n",
    "    while len(hash_set) < target:\n",
    "        g = generator()\n",
    "\n",
    "        score, partial_scores, values = calc_score(g, total_cont=len(hash_set), invariant_occ=invariant_occurrences)\n",
    "\n",
    "        global_scores.append(score)\n",
    "        accepted = false\n",
    "\n",
    "        if score >= min_score(score_list['general'], w_size=window_size):\n",
    "            accepted = add_graph_list(g, invariant_values=values)\n",
    "            if not accepted:\n",
    "                already_existing_count += 1\n",
    "            \n",
    "        accepted_mask.append(1 if accepted else 0)\n",
    "\n",
    "        score_list['general'].append(score)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            new_valid_count = len(hash_set)\n",
    "            elapsed_time = time.time() - start\n",
    "            generator_index = (generator_index + 1) % len(generators)\n",
    "            \n",
    "            log_message = (\n",
    "                f\"Previous generator: {generator.__name__}\\n\"\n",
    "                f\"Iteration: {i}\\n\"\n",
    "                f\"Elapsed time: {elapsed_time} seconds\\n\"\n",
    "                f\"Graphs added: {new_valid_count - old_valid_count}\\n\"\n",
    "                f\"Valid graph ratio: {round(((new_valid_count - old_valid_count)/10),2)}%\\n\"\n",
    "                f\"Rejected graphs (already existing): {already_existing_count}\\n\"\n",
    "                f\"Dataset size: {len(hash_set)}\\n\\n\"\n",
    "            )\n",
    "\n",
    "            out_file.write(log_message)\n",
    "            out_file.flush()  # to write in real time\n",
    "\n",
    "            generator = generators[generator_index]\n",
    "            old_valid_count = new_valid_count\n",
    "            already_existing_count = 0\n",
    "            print(i)\n",
    "            start = time.time()\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            save_checkpoint(\n",
    "                graph_list=graph_list, \n",
    "                invariant_occurrences=invariant_occurrences, \n",
    "                global_scores=global_scores,\n",
    "                accepted_mask=accepted_mask,\n",
    "                checkpoint_dir='../data/checkpoints', \n",
    "                index=checkpoint_index,\n",
    "                hash_set=hash_set\n",
    "            )\n",
    "            checkpoint_index += 1\n",
    "            graph_list = []\n",
    "    \n",
    "    save_checkpoint(\n",
    "        graph_list=graph_list, \n",
    "        invariant_occurrences=invariant_occurrences, \n",
    "        global_scores=global_scores,\n",
    "        accepted_mask=accepted_mask,\n",
    "        checkpoint_dir='../data/checkpoints', \n",
    "        index=checkpoint_index\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list, invariant_occurrences, global_scores, accepted_mask, hash_set = restore_checkpoint(checkpoint_dir=\"../data/checkpoints\")\n",
    "print(len(graph_list))\n",
    "print(invariant_occurrences)\n",
    "print(len(global_scores))\n",
    "print(len(accepted_mask))\n",
    "print(np.array(accepted_mask).sum())\n",
    "print(len(hash_set))\n",
    "print(np.where(np.array(global_scores) < 0)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_graphs_to_g6(graphs, filepath):\n",
    "    with open(filepath, \"a\") as f:\n",
    "        for i, G in enumerate(graphs):\n",
    "            g6_str = nx.to_graph6_bytes(G, header=False).decode(\"utf-8\").strip()\n",
    "            f.write(g6_str + \"\\n\")\n",
    "            print(i)\n",
    "            \n",
    "\n",
    "def write_database_to_file(folder_path, out_path):\n",
    "    with open(out_path, \"w\") as outfile:\n",
    "        for i in range(2, 9):\n",
    "            with open(os.path.join(folder_path, f\"graph{i}.g6\"), \"r\") as database_file:\n",
    "                outfile.write(database_file.read())    \n",
    "            outfile.write('\\n')\n",
    "\n",
    "write_database_to_file(folder_path=\"../../data/database\", out_path=\"../data/raw/dataset.g6\")\n",
    "append_graphs_to_g6(graphs=graph_list, filepath=\"../data/raw/dataset.g6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored checkpoints : 47.25639796257019 s\n",
      "Parsing graph5.g6\n",
      "Parsing graph4.g6\n",
      "Parsing graph3.g6\n",
      "Parsing graph7.g6\n",
      "Parsing graph6.g6\n",
      "Parsing graph2.g6\n",
      "Parsing graph8.g6\n",
      "parsed complete database (2 - 8) : 23.563014268875122 s\n",
      "lists merged : 0.0030128955841064453 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'networkx.utils.decorators.argmap'> compilation 9:4: FutureWarning: laplacian_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "/var/folders/xr/j1k993bs101gs699fxggjf3h0000gn/T/ipykernel_50789/3304937154.py:36: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  real_value = max(abs(np.linalg.eigvals(nx.adjacency_matrix(G).toarray())))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000 graphs : 37.53977966308594 s\n",
      "processed 20000 graphs : 36.37114763259888 s\n",
      "processed 30000 graphs : 34.88269329071045 s\n",
      "processed 40000 graphs : 35.70436930656433 s\n",
      "processed 50000 graphs : 38.33825206756592 s\n",
      "processed 60000 graphs : 35.98636293411255 s\n",
      "processed 70000 graphs : 66.27533721923828 s\n",
      "processed 80000 graphs : 41.00601387023926 s\n",
      "processed 90000 graphs : 32.133325815200806 s\n",
      "processed 100000 graphs : 35.81530809402466 s\n",
      "processed 110000 graphs : 31.577635049819946 s\n",
      "processed 120000 graphs : 37.96307897567749 s\n",
      "processed 130000 graphs : 37.55972719192505 s\n",
      "processed 140000 graphs : 65.37597918510437 s\n",
      "processed 150000 graphs : 35.36451005935669 s\n",
      "processed 160000 graphs : 35.31768012046814 s\n",
      "processed 170000 graphs : 39.59867477416992 s\n",
      "processed 180000 graphs : 35.51334190368652 s\n",
      "processed 190000 graphs : 40.21569299697876 s\n",
      "processed 200000 graphs : 34.30967903137207 s\n",
      "processed 210000 graphs : 33.66223096847534 s\n",
      "processed 220000 graphs : 65.19614386558533 s\n",
      "processed 230000 graphs : 37.41539216041565 s\n",
      "processed 240000 graphs : 36.72486996650696 s\n",
      "processed 250000 graphs : 32.00699996948242 s\n",
      "processed 260000 graphs : 35.30618500709534 s\n",
      "processed 270000 graphs : 39.40112376213074 s\n",
      "processed 280000 graphs : 38.71686410903931 s\n",
      "processed 290000 graphs : 71.38065886497498 s\n",
      "processed 300000 graphs : 32.06127691268921 s\n",
      "processed 310000 graphs : 6.243441104888916 s\n",
      "dictionary created\n",
      "../data/dataset/graphs2.json written : 2.468679904937744 s\n",
      "../data/dataset/graphs3.json written : 0.00019788742065429688 s\n",
      "../data/dataset/graphs4.json written : 0.0002498626708984375 s\n",
      "../data/dataset/graphs5.json written : 0.001219034194946289 s\n",
      "../data/dataset/graphs6.json written : 0.0033767223358154297 s\n",
      "../data/dataset/graphs7.json written : 0.022306203842163086 s\n",
      "../data/dataset/graphs8.json written : 0.316972017288208 s\n",
      "../data/dataset/graphs9.json written : 0.13740015029907227 s\n",
      "../data/dataset/graphs10.json written : 0.13879084587097168 s\n",
      "../data/dataset/graphs11.json written : 0.21518278121948242 s\n",
      "../data/dataset/graphs12.json written : 0.19225597381591797 s\n",
      "../data/dataset/graphs13.json written : 0.23514604568481445 s\n",
      "../data/dataset/graphs14.json written : 0.25617384910583496 s\n",
      "../data/dataset/graphs15.json written : 0.28773999214172363 s\n",
      "../data/dataset/graphs16.json written : 0.31221699714660645 s\n",
      "../data/dataset/graphs17.json written : 0.3520090579986572 s\n",
      "../data/dataset/graphs18.json written : 0.39250612258911133 s\n",
      "../data/dataset/graphs19.json written : 0.4885740280151367 s\n",
      "../data/dataset/graphs20.json written : 0.5005261898040771 s\n",
      "../data/dataset/graphs21.json written : 0.5784599781036377 s\n",
      "../data/dataset/graphs22.json written : 0.6391150951385498 s\n",
      "../data/dataset/graphs23.json written : 0.752730131149292 s\n",
      "../data/dataset/graphs24.json written : 0.8248579502105713 s\n",
      "../data/dataset/graphs25.json written : 0.9765989780426025 s\n",
      "../data/dataset/graphs26.json written : 1.0585730075836182 s\n",
      "../data/dataset/graphs27.json written : 1.1905829906463623 s\n",
      "../data/dataset/graphs28.json written : 1.2987241744995117 s\n",
      "../data/dataset/graphs29.json written : 1.4685430526733398 s\n",
      "../data/dataset/graphs30.json written : 1.6053087711334229 s\n",
      "../data/dataset/graphs31.json written : 1.7707719802856445 s\n",
      "../data/dataset/graphs32.json written : 1.9387338161468506 s\n",
      "../data/dataset/graphs33.json written : 2.1312026977539062 s\n",
      "../data/dataset/graphs34.json written : 2.2843971252441406 s\n",
      "../data/dataset/graphs35.json written : 2.450782060623169 s\n",
      "../data/dataset/graphs36.json written : 2.7047059535980225 s\n",
      "../data/dataset/graphs37.json written : 2.9603610038757324 s\n",
      "../data/dataset/graphs38.json written : 3.118407964706421 s\n",
      "../data/dataset/graphs39.json written : 3.41854190826416 s\n",
      "../data/dataset/graphs40.json written : 3.541836738586426 s\n",
      "../data/dataset/graphs41.json written : 3.9752588272094727 s\n",
      "../data/dataset/graphs42.json written : 4.161778211593628 s\n",
      "../data/dataset/graphs43.json written : 4.5059239864349365 s\n",
      "../data/dataset/graphs44.json written : 4.8342437744140625 s\n",
      "../data/dataset/graphs45.json written : 5.157718896865845 s\n",
      "../data/dataset/graphs46.json written : 5.645231008529663 s\n",
      "../data/dataset/graphs47.json written : 5.84520697593689 s\n",
      "../data/dataset/graphs48.json written : 6.3125081062316895 s\n",
      "../data/dataset/graphs49.json written : 6.583880186080933 s\n",
      "../data/dataset/graphs50.json written : 8.382812023162842 s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import os\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "def reset_time_with_message(message, t):\n",
    "    print(f\"{message} : {time.time() - t} s\")\n",
    "    return time.time()\n",
    "\n",
    "def parse_g6_directory(directory: str):\n",
    "    graphs = []\n",
    "    if not os.path.isdir(directory):\n",
    "        raise FileNotFoundError(f\"Directory '{directory}' not found.\")\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".g6\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f\"Parsing {filename}\")\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        try:\n",
    "                            graph = nx.from_graph6_bytes(line.encode())\n",
    "                            graphs.append(graph)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error parsing {filename}: {e}\")    \n",
    "    return graphs\n",
    "\n",
    "\n",
    "def spectral_radius_lm(G):\n",
    "    real_value = max(abs(np.linalg.eigvals(nx.laplacian_matrix(G).toarray())))\n",
    "    return real_value\n",
    "\n",
    "def spectral_radius_am(G):\n",
    "    real_value = max(abs(np.linalg.eigvals(nx.adjacency_matrix(G).toarray())))\n",
    "    return real_value\n",
    "\n",
    "funcs[\"spectral_radius_adjacency\"] = spectral_radius_am\n",
    "funcs[\"spectral_radius_laplacian\"] = spectral_radius_lm\n",
    "\n",
    "t = time.time()\n",
    "nx_list, _, _, _, _ = restore_checkpoint(checkpoint_dir=\"../data/checkpoints\")\n",
    "t = reset_time_with_message(message=\"restored checkpoints\", t=t)\n",
    "databse_graphs = parse_g6_directory(\"../../data/database\")\n",
    "t = reset_time_with_message(message=\"parsed complete database (2 - 8)\", t=t)\n",
    "nx_list.extend(databse_graphs)\n",
    "t = reset_time_with_message(message=\"lists merged\", t=t)\n",
    "\n",
    "graphs_lists = {n_nodes: [] for n_nodes in range(2, 51)}\n",
    "for i, G in enumerate(nx_list):\n",
    "    invariants = calc_invariants(G)\n",
    "    invariant_values = [invariants[key] for key in funcs.keys()]\n",
    "    graph_data = {\n",
    "        \"graph_features\": invariant_values,\n",
    "        \"nodes\": list(G.nodes()),\n",
    "        \"edges\": list(G.edges())\n",
    "    }\n",
    "    graphs_lists[invariants['nodes_count']].append(graph_data)\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        t = reset_time_with_message(message=f\"processed {i + 1} graphs\", t=t)\n",
    "\n",
    "print(\"dictionary created\")\n",
    "\n",
    "for n_nodes in graphs_lists.keys():\n",
    "    data = {\n",
    "        'nodes_count': n_nodes,\n",
    "        'invariants_order': list(funcs.keys()),\n",
    "        'graph_list': graphs_lists[n_nodes]\n",
    "    }\n",
    "    with open(f\"../data/dataset/graphs{n_nodes}.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "    t = reset_time_with_message(message=f\"../data/dataset/graphs{n_nodes}.json written\", t=t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def read_parse_json_files(directory_path):\n",
    "    graphs = []\n",
    "    i = 0\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.json'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"{i + 1} reading {file_path}\")\n",
    "            i += 1\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "                    graphs.extend(data['graph_list'])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing {filename}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "graphs = read_parse_json_files('../data/dataset')\n",
    "print(len(graphs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
